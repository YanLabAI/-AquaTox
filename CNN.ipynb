{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be66193c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T13:30:52.206561Z",
     "start_time": "2021-05-05T13:28:22.551369Z"
    },
    "code_folding": [
     114,
     117,
     129,
     530,
     665
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "nets = [\"alexnet\", \"vgg19_bn\", \"resnet152\", \"densenet201\",  \"resnet152_extended\", \"alexnet_extended\", \"densenet201_extended\", \"resnet34\",\"resnet34_extended\", \"densenet121_extended\"]\n",
    "tox = [\"IGC50\", \"IBC50\", \"LC50\", \"LC50DM\"]\n",
    "argparser = ArgumentParser()\n",
    "args_group = argparser.add_argument_group(title='Running args')\n",
    "args_group.add_argument('-seed', type=int, help='Seed used to split the data into training, validation and test sets', required=False, default=1)\n",
    "args_group.add_argument('-task', type=str, help='Set this argument to train in order to train the network, or to predict to load a pretrained model', required=False, default=\"train\",choices=['train','predict'])\n",
    "args_group.add_argument('-architecture', type=str, help='ConvNet to be used', required=False, choices=nets, default=\"vgg19_bn\")\n",
    "args_group.add_argument('-lr', type=float, help='Maximum learning rate value. Please note that optimal values vary across architectures', required=False, default=0.01)\n",
    "args_group.add_argument('-step_size_lr_decay', type=int, help='Step size to decrease the learning rate by a given factor (parameter drop_factor_lr)', required=False, default=25)\n",
    "args_group.add_argument('-drop_factor_lr', type=float, help='The learning rate is reduced by the factor indicated in this argument', required=False, default=0.6 )\n",
    "args_group.add_argument('-batch_size', type=int, help='Batch size', required=False, default=16)\n",
    "args_group.add_argument('-data_augmentation', type=int, help='Whether data augmentation should be applied to the validation and training sets (1: yes; 0: no)', required=False, default=1,choices=[0,1] )\n",
    "args_group.add_argument('-nb_epochs_training_per_cycle', type=int, help='Number of epochs for each learning rate annealing cycle', required=False, default=200)\n",
    "args_group.add_argument('-nb_epochs_training', type=int, help='Number of epochs to be considered for training', required=False, default=600)\n",
    "args_group.add_argument('-epochs_early_stop', type=int, help='Number of epochs for early stopping', required=False, default=250)\n",
    "args_group.add_argument('-load_dict', type=list, help='load_dict', required=False, nargs='+', default=[])\n",
    "args_group.add_argument('-Tox', type=str, help='Tox', required=False, default=0)\n",
    "args_group.add_argument('-cv', type=int, help='cv', required=False, default=5)\n",
    "args = argparser.parse_args(['-batch_size', '16'\n",
    "                             , \"-architecture\"\n",
    "                             , \"densenet201_extended\"\n",
    "                             ,'-load_dict','0','0','0','0','0'\n",
    "                             ,'-nb_epochs_training', '300'\n",
    "                             ,'-lr', '0.01'\n",
    "                             ,'-nb_epochs_training_per_cycle', '100'\n",
    "                             ,'-Tox', 'IGC50'\n",
    "                             ,'-cv', '5'])\n",
    "\n",
    "\n",
    "seed = args.seed\n",
    "net=args.architecture\n",
    "lr=args.lr\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "import os, glob, time\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os,sys, os.path\n",
    "from collections import defaultdict\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import rdkit.rdBase\n",
    "from rdkit import DataStructs\n",
    "from rdkit.DataStructs import BitVectToText\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG\n",
    "\n",
    "from IPython.core.display import SVG\n",
    "from torch.autograd import Variable\n",
    "import multiprocessing\n",
    "\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from rdkit.Chem import Draw\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def default_flist_reader(flist):\n",
    "    imlist = []\n",
    "    with open(flist, 'r') as rf:\n",
    "        for line in rf.readlines():\n",
    "            impath, imlabel = line.strip().split()\n",
    "            imlist.append( (impath, int(imlabel)) )\n",
    "    \n",
    "    return imlist\n",
    "\n",
    "class ImageFilelist(data.Dataset):\n",
    "    def __init__(self,  paths_labels, transform=None, target_transform=None,\n",
    "        flist_reader=default_flist_reader, loader=default_loader):\n",
    "        self.imlist = paths_labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        impath, target = self.imlist[index]\n",
    "        img = self.loader(impath)\n",
    "        if self.transform is not None:\n",
    "           img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "           target = self.target_transform(target)\n",
    "   \n",
    "        return img, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imlist)\n",
    "\n",
    "mols_train = []\n",
    "mols_test = []\n",
    "suppl_train = Chem.SDMolSupplier(\"./data/{}/{}_training.sdf\".format(args.Tox, args.Tox))\n",
    "suppl_pre = Chem.SDMolSupplier(\"./data/{}/{}_prediction.sdf\".format(args.Tox, args.Tox))\n",
    "for mol in suppl_train:\n",
    "    mols_train.append(mol)\n",
    "for mol in suppl_pre:\n",
    "    mols_test.append(mol)\n",
    "print(len(mols_train),len(mols_test))\n",
    "if len(suppl_train)+len(suppl_pre) == len(mols_train)+len(mols_test):\n",
    "    print(\"mols ready\")\n",
    "\n",
    "my_smiles_train=[Chem.MolToSmiles(submol) for submol in mols_train]\n",
    "my_smiles_test=[Chem.MolToSmiles(submol) for submol in mols_test]\n",
    "chembl_ids_train=[m.GetProp(\"CAS\") for m in mols_train]\n",
    "chembl_ids_test=[m.GetProp(\"CAS\") for m in mols_test]\n",
    "activities_train =[float(m.GetProp(\"Tox\")) for m in mols_train]\n",
    "activities_test =[float(m.GetProp(\"Tox\")) for m in mols_test]\n",
    "\n",
    "if(len(my_smiles_train)+len(my_smiles_test) != len(activities_train)+len(activities_test)):\n",
    "    raise \"The number of compounds does not correspond to the number of bioactivities\"\n",
    "\n",
    "base_indices = np.arange(0,len(activities_train))\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(base_indices)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(base_indices)\n",
    "\n",
    "#-----------------------------------------------\n",
    "# data augmentation\n",
    "#-----------------------------------------------\n",
    "\n",
    "if args.data_augmentation == 1:\n",
    "    transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            }\n",
    "else:\n",
    "    transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            }\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "# Data loaders\n",
    "#------------------------------------\n",
    "\n",
    "paths_labels_train=[]\n",
    "for i,x in enumerate(activities_train):\n",
    "    path_now = './images/{}/train/{}.png'.format(args.Tox, chembl_ids_train[i])\n",
    "    now = (path_now , x)\n",
    "    paths_labels_train.append(now)\n",
    "\n",
    "paths_labels_test=[]\n",
    "for i,x in enumerate(activities_test):\n",
    "    path_now = './images/{}/test/{}.png'.format(args.Tox, chembl_ids_test[i])\n",
    "    now = (path_now , x)\n",
    "    paths_labels_test.append(now)                                               \n",
    "     \n",
    "\n",
    "workers=multiprocessing.cpu_count()\n",
    "workers = 0\n",
    "shuffle=False\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#-----------------------------------------------\n",
    "# Training the model\n",
    "#-----------------------------------------------\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    print(\"-\"*20)\n",
    "    print(\"Strat Training {} {} cv: {}\".format('Aquatox', args.Tox, cv+1))\n",
    "    print(net)\n",
    "    print(\"data_augmentation: {}\".format(args.data_augmentation))\n",
    "    print(\"batch_size: {}\".format(args.batch_size))\n",
    "    start_epoch = -1\n",
    "    best_epoch = 0\n",
    "    load_epoch = int(args.load_dict[cv][0])\n",
    "    if  load_epoch > 0:\n",
    "        checkpoint = torch.load(\"./models/checkpoint/{}/{}/{}/{}_{}.pth\".format(args.Tox,args.seed,net,load_epoch, cv))  # 加载断点\n",
    "        print(\"strat from\",checkpoint[\"epoch\"],\"epoch\")\n",
    "        \n",
    "        best_epoch = load_epoch\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        scheduler.load_state_dict(checkpoint['lr_schedule'])\n",
    "        net_dict = copy.deepcopy(checkpoint['net'])\n",
    "        model.load_state_dict(net_dict)\n",
    "        del net_dict,checkpoint\n",
    "        \n",
    "    print(\"-\"*20)\n",
    "    model.cuda()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1000.0\n",
    "    best_r2 = -10000\n",
    "    early = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch+1, num_epochs):\n",
    "        time_epoch = time.time()\n",
    "\n",
    "        # cyclical learning rate\n",
    "        if early % args.nb_epochs_training_per_cycle == 0:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "        \n",
    "        print(\"-\"*20)\n",
    "        print('Epoch {}/{} early:{}'.format(epoch, num_epochs - 1, early))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            epoch_losses=0.0\n",
    "            deno=0.0\n",
    "            if phase == 'train':\n",
    "#                 scheduler.step()\n",
    "                model.train()  \n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        preds=outputs.squeeze(1)\n",
    "                        preds = preds.type(torch.FloatTensor)\n",
    "                        loss = criterion(preds, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    del inputs, outputs, labels\n",
    "                    epoch_losses += loss.item() * len(preds)  #data[0]\n",
    "                    deno +=len(preds)\n",
    "                    del preds\n",
    "                    \n",
    "                epoch_loss = epoch_losses / deno\n",
    "                train_loss.append(epoch_loss)\n",
    "                print('{} Loss: {:.4f} {}'.format(phase, epoch_loss, deno))\n",
    "                \n",
    "            if phase == 'val':\n",
    "                model.eval()\n",
    "                pred=[]\n",
    "                obs=[]\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    outputs = model(inputs)\n",
    "                    for i in range(len(labels)):\n",
    "                        pred.append(float(outputs.data[i]))\n",
    "                        obs.append(float(labels.data[i]))\n",
    "                \n",
    "                    del inputs, outputs, labels\n",
    "\n",
    "                mse = mean_squared_error(obs, pred)\n",
    "                r2 = r2_score(obs, pred)\n",
    "                lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "                print('val Loss: {:.4f} r^2: {:.4f} lr: {:.4f}'.format(mse, r2, lr))\n",
    "                lr_decay.append(lr)\n",
    "                scores_mse.append(mse)\n",
    "                scores_r2.append(r2)\n",
    "                \n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                early=0\n",
    "                \n",
    "                kf_pred_all.iloc[index_val, 0] = pred\n",
    "                kf_pred_all.iloc[index_val, 1] = obs\n",
    "                \n",
    "                print('save at:',epoch)\n",
    "                checkpoint = {\n",
    "                            \"net\": model.state_dict(),\n",
    "                            'optimizer':optimizer.state_dict(),\n",
    "                            \"epoch\": epoch,\n",
    "                            \"lr_schedule\": scheduler.state_dict()\n",
    "                            }\n",
    "                if not os.path.isdir(\"./models/checkpoint/{}/{}/{}\".format(args.Tox,args.seed,net)):\n",
    "                    os.makedirs(\"./models/checkpoint/{}/{}/{}\".format(args.Tox,args.seed,net),exist_ok=True)\n",
    "                torch.save(checkpoint, \"./models/checkpoint/{}/{}/{}/{}_{}.pth\".format(args.Tox,args.seed,net,epoch, cv))\n",
    "                if epoch >0:\n",
    "                    os.remove(\"./models/checkpoint/{}/{}/{}/{}_{}.pth\".format(args.Tox,args.seed,net,best_epoch, cv))\n",
    "                best_epoch = epoch\n",
    "                \n",
    "        \n",
    "            if phase == 'val' and r2 < best_r2:\n",
    "                early+=1\n",
    "      \n",
    "            if phase == 'train' and early>20:\n",
    "                scheduler.step()\n",
    "\n",
    "        print('Epoch complete in {:.0f}m {:.0f}s'.format( (time.time() - time_epoch) // 60, (time.time() - time_epoch) % 60))\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best r2: {:4f}'.format(best_r2))\n",
    "    print(\"-\"*20)\n",
    "    print(\"end\")\n",
    "    print(\"-\"*20)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    print(\"-\"*20)\n",
    "    print(\"Strat Training Test {} {}\".format('Aquatox', args.Tox))\n",
    "    print(net)\n",
    "    print(\"data_augmentation: {}\".format(args.data_augmentation))\n",
    "    print(\"batch_size: {}\".format(args.batch_size))\n",
    "    start_epoch = -1\n",
    "    best_epoch = 0\n",
    "    print(\"-\"*20)\n",
    "    model.cuda()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1000.0\n",
    "    best_r2 = -10000\n",
    "    early = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch+1, num_epochs):\n",
    "        time_epoch = time.time()\n",
    "\n",
    "        if early % args.nb_epochs_training_per_cycle == 0:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "        \n",
    "        print(\"-\"*20)\n",
    "        print('Epoch {}/{} early:{}'.format(epoch, num_epochs - 1, early))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            epoch_losses=0.0\n",
    "            deno=0.0\n",
    "            if phase == 'train':\n",
    "#                 scheduler.step()\n",
    "                model.train()  \n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        preds=outputs.squeeze(1)\n",
    "                        preds = preds.type(torch.FloatTensor)\n",
    "                        loss = criterion(preds, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    del inputs, outputs, labels\n",
    "                    epoch_losses += loss.item() * len(preds)  #data[0]\n",
    "                    deno +=len(preds)\n",
    "                    del preds\n",
    "                    \n",
    "                epoch_loss = epoch_losses / deno\n",
    "                train_loss.append(epoch_loss)\n",
    "                print('{} Loss: {:.4f} {}'.format(phase, epoch_loss, deno))\n",
    "                \n",
    "            if phase == 'val':\n",
    "                model.eval()\n",
    "                pred=[]\n",
    "                obs=[]\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    outputs = model(inputs)\n",
    "                    for i in range(len(labels)):\n",
    "                        pred.append(float(outputs.data[i]))\n",
    "                        obs.append(float(labels.data[i]))\n",
    "                \n",
    "                    del inputs, outputs, labels\n",
    "\n",
    "                mse = mean_squared_error(obs, pred)\n",
    "                r2 = r2_score(obs, pred)\n",
    "                lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "                print('val Loss: {:.4f} r^2: {:.4f} lr: {:.4f}'.format(mse, r2, lr))\n",
    "                lr_decay.append(lr)\n",
    "                scores_mse.append(mse)\n",
    "                scores_r2.append(r2)\n",
    "                \n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            if phase == 'val' and r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                early=0\n",
    "                \n",
    "                test_pred.iloc[:, 0] = pred\n",
    "                test_pred.iloc[:, 1] = obs\n",
    "                \n",
    "                print('save at:',epoch)\n",
    "        \n",
    "            if phase == 'val' and r2 < best_r2:\n",
    "                early+=1            \n",
    "                \n",
    "            if phase == 'train' and early>20:\n",
    "                scheduler.step()\n",
    "\n",
    "        print('Epoch complete in {:.0f}m {:.0f}s'.format( (time.time() - time_epoch) // 60, (time.time() - time_epoch) % 60))\n",
    "    \n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best r2: {:4f}'.format(best_r2))\n",
    "    print(\"-\"*20)\n",
    "    print(\"end\")\n",
    "    print(\"-\"*20)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Architectures\n",
    "#-----------------------------------------------\n",
    "\n",
    "def modelselect(net):\n",
    "    if net not in nets:\n",
    "        raise \"The selected architecture is not available\"\n",
    "\n",
    "    if net == \"alexnet\": \n",
    "        model_ft = models.alexnet(pretrained=False)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, 1)\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"alexnet_extended\": \n",
    "        model_ft = models.alexnet(pretrained=False)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=9216, out_features=4096, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=4096, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_ft.classifier = classi\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"densenet201\": \n",
    "        model_ft = models.densenet201(pretrained=False)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, 1)\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)#, momentum=0.95) #, nesterov=True)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"densenet201_extended\": \n",
    "        model_ft = models.densenet201(pretrained=False)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=1920, out_features=4096, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=4096, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_ft.classifier = classi\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"vgg19_bn\": \n",
    "        model_ft = models.vgg19_bn(pretrained=False)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=25088, out_features=4096, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=4096, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_ft.classifier = classi\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)#, momentum=0.95) #, nesterov=True)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"resnet152\": \n",
    "        model_ft = models.resnet152(pretrained=False)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)#, momentum=0.95) #, nesterov=True)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"resnet152_extended\": \n",
    "        model_ft = models.resnet152(pretrained=False)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=2048, out_features=4096, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=4096, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_ft.fc = classi\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"resnet34_extended\": \n",
    "        model_ft = models.resnet152(pretrained=False)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=2048, out_features=4096, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=4096, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_ft.fc = classi\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    if net == \"densenet121_extended\": \n",
    "        model_ft = models.densenet121(pretrained=False)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=1024, out_features=2048, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=2048, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_ft.classifier = classi\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "    return model_ft\n",
    "\n",
    "for net in [\"vgg19_bn\", \"resnet152_extended\",\"densenet201_extended\"]:\n",
    "\n",
    "    #-----------------------------------------------\n",
    "    # Training\n",
    "    #-----------------------------------------------\n",
    "    r2_all = []\n",
    "    lr_all = []\n",
    "    mse_all = []\n",
    "    kf_pred_all = pd.DataFrame(index=range(len(mols_train)), columns=[\"cv1~5\", \"True\"])\n",
    "    test_score_all = pd.DataFrame(index=list(range(len(mols_test)))+[\"r2\",\"MSE\"], columns=[\"cv1\", \"cv2\", \"cv3\", \"cv4\", \"cv5\", \"True\"])\n",
    "    train_loss_all = []\n",
    "    if args.task==\"train\":\n",
    "        index = base_indices\n",
    "        step = int(len(index)/args.cv)\n",
    "        for cv in range(args.cv):\n",
    "            model_ft = modelselect(net)\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "            scores_r2 = []\n",
    "            lr_decay = []\n",
    "            scores_mse = []\n",
    "            train_loss = []\n",
    "            if cv < args.cv-1:\n",
    "                index_train = np.concatenate([index[:cv*step],index[(cv+1)*step:]], axis=0)\n",
    "                index_val = index[cv*step:(cv+1)*step]\n",
    "            else: \n",
    "                index_train = index[0:cv*step]\n",
    "                index_val = index[cv*step:]\n",
    "\n",
    "            paths_labels_train_train = []\n",
    "            for i in index_train:\n",
    "                paths_labels_train_train.append(paths_labels_train[i])\n",
    "\n",
    "            paths_labels_train_val = []\n",
    "            for i in index_val:\n",
    "                paths_labels_train_val.append(paths_labels_train[i])\n",
    "                \n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                        ImageFilelist(paths_labels= paths_labels_train_train,\n",
    "                        transform=transforms['train']),\n",
    "                        batch_size=args.batch_size, shuffle=shuffle,\n",
    "                        num_workers=workers) \n",
    "\n",
    "            valloader = torch.utils.data.DataLoader(\n",
    "                        ImageFilelist(paths_labels= paths_labels_train_val,\n",
    "                        transform=transforms['val']),\n",
    "                        batch_size=args.batch_size, shuffle=shuffle,\n",
    "                        num_workers=workers) \n",
    "\n",
    "            testloader = torch.utils.data.DataLoader(\n",
    "                        ImageFilelist(paths_labels= paths_labels_test,\n",
    "                        transform=transforms['test']),\n",
    "                        batch_size=args.batch_size, shuffle=shuffle,\n",
    "                        num_workers=workers) \n",
    "\n",
    "            dataloaders = {'train': trainloader, 'val':valloader, 'test':testloader}\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "            model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=args.nb_epochs_training)\n",
    "\n",
    "            pred = []\n",
    "            obs = []\n",
    "            for inputs, labels in dataloaders['test']:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                model_ft.cuda()\n",
    "                labels = labels.type(torch.FloatTensor)\n",
    "                outputs = model_ft(inputs)\n",
    "                for i in range(len(labels)):\n",
    "                    pred.append(float(outputs.data[i]))\n",
    "                    obs.append(float(labels.data[i]))\n",
    "\n",
    "                del inputs, outputs, labels\n",
    "\n",
    "            mse = mean_squared_error(obs, pred)\n",
    "            r2 = r2_score(obs, pred)\n",
    "            test_score_all.iloc[:len(pred),cv] = pred\n",
    "            test_score_all.iloc[:len(obs),args.cv] = obs\n",
    "            test_score_all.iloc[len(pred),cv] = r2\n",
    "            test_score_all.iloc[len(pred)+1,cv] = mse\n",
    "            print('test Loss: {:.4f} r^2: {}'.format(mse, r2))\n",
    "\n",
    "\n",
    "            r2_all.append(scores_r2)\n",
    "            lr_all.append(lr_decay)\n",
    "            mse_all.append(scores_mse)\n",
    "            train_loss_all.append(train_loss)\n",
    "        \n",
    "        os.makedirs(\"./results/{}/DL/{}/{}/train\".format(args.Tox, args.seed,net), exist_ok=True)\n",
    "        os.makedirs(\"./results/{}/DL/{}/{}/test\".format(args.Tox, args.seed,net), exist_ok=True)\n",
    "        pd.DataFrame(r2_all).T.to_csv(\"./results/{}/DL/{}/{}/train/r2_all.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "        pd.DataFrame(lr_all).T.to_csv(\"./results/{}/DL/{}/{}/train/lr_all.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "        pd.DataFrame(mse_all).T.to_csv(\"./results/{}/DL/{}/{}/train/mse_all.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "        pd.DataFrame(kf_pred_all).to_csv(\"./results/{}/DL/{}/{}/train/kf_pred_all.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "        pd.DataFrame(train_loss_all).T.to_csv(\"./results/{}/DL/{}/{}/train/train_loss_all.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "        pd.DataFrame(test_score_all).to_csv(\"./results/{}/DL/{}/{}/test/test_score_all.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "        \n",
    "    ## use all train to test\n",
    "    scores_r2 = []\n",
    "    lr_decay = []\n",
    "    scores_mse = []\n",
    "    train_loss = []\n",
    "    test_pred = pd.DataFrame(index=range(len(activities_test)), columns=[\"pred\", \"True\"])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                ImageFilelist(paths_labels= paths_labels_train,\n",
    "                transform=transforms['train']),\n",
    "                batch_size=args.batch_size, shuffle=shuffle,\n",
    "                num_workers=workers) \n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "                ImageFilelist(paths_labels= paths_labels_test,\n",
    "                transform=transforms['val']),\n",
    "                batch_size=args.batch_size, shuffle=shuffle,\n",
    "                num_workers=workers) \n",
    "\n",
    "    dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "    model_ft = modelselect(net)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=args.lr)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "    model_ft = train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=args.nb_epochs_training)\n",
    "\n",
    "    pd.DataFrame(scores_r2).to_csv(\"./results/{}/DL/{}/{}/test/scores_r2.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "    pd.DataFrame(lr_decay).to_csv(\"./results/{}/DL/{}/{}/test/lr_decay.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "    pd.DataFrame(scores_mse).to_csv(\"./results/{}/DL/{}/{}/test/scores_mse.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "    pd.DataFrame(train_loss).to_csv(\"./results/{}/DL/{}/{}/test/train_loss.csv\".format(args.Tox, args.seed,net),index=None)\n",
    "    pd.DataFrame(test_pred).to_csv(\"./results/{}/DL/{}/{}/test/test_pred.csv\".format(args.Tox, args.seed,net),index=None)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.0 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "77b022008d7e49a7577e3989aa2de88606942ee75ef8e6a397bf4b790024deae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
